@article{lewis2020retrieval,
  author    = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich K{\"u}ttler and Mike Lewis and Wen-tau Yih and Tim Rockt{\"a}schel and Sebastian Riedel and Douwe Kiela},
  title     = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {33},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.11401}
}

@article{gemini2024,
  author    = {Gemini Team},
  title     = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  journal   = {arXiv preprint arXiv:2403.05530},
  year      = {2024},
  url       = {https://arxiv.org/abs/2403.05530}
}

@misc{drope2025,
  author    = {{Sakana AI Research Team}},
  title     = {DroPE: Extending the Context of Pretrained LLMs by Dropping their Positional Embeddings},
  year      = {2025},
  howpublished = {\url{https://pub.sakana.ai/DroPE}},
  note      = {arXiv:2512.12167}
}

@misc{gemini2025updates,
  author    = {{Google DeepMind}},
  title     = {Gemini 2.0 Model Updates},
  year      = {2025},
  howpublished = {\url{https://blog.google/innovation-and-ai/models-and-research/google-deepmind/gemini-model-updates-february-2025}}
}

@article{reid2024gemini,
  author    = {Machel Reid and Nikolay Savinov and Denis Teplyashin and Dmitry Lepikhin and Timothy Lillicrap and Jean-Baptiste Alayrac and Radu Soricut and Angeliki Lazaridou and Orhan Firat and Julian Schrittwieser and Ioannis Antonoglou and Rohan Anil and Sebastian Borgeaud and Andrew Dai and Katie Millican and Ethan Dyer and Mia Glaese and Thibault Sottiaux and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and James Molloy and Jilin Chen and Michael Isard and Paul Barham and Tom Hennigan and Ross McIlroy and Melvin Johnson and Johan Schalkwyk and Eli Collins and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and Clemens Meyer and Gregor (and 600+ others from the Gemini Team)},
  title     = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  journal   = {arXiv preprint arXiv:2403.05530},
  year      = {2024},
  url       = {https://arxiv.org/abs/2403.05530},
  note      = {Updated versions through 2025; lead author Machel Reid in initial releases}
}

@article{ett2025,
  author    = {Kiarash Zahirnia and Zahra Golpayegani and Walid Ahmed and Yang Liu},
  title     = {ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time},
  journal   = {arXiv preprint arXiv:2507.06313},
  year      = {2025},
  url       = {https://arxiv.org/abs/2507.06313}
}

@misc{contextrot2025,
  author    = {{Chroma Research Team}},
  title     = {Context Rot: How Increasing Input Tokens Impacts LLM Performance},
  year      = {2025},
  howpublished = {\url{https://research.trychroma.com/context-rot}},
  note      = {Technical report, July 14, 2025}
}

@article{squeezed2025,
  author    = {{SqueezeAI Lab}},
  title     = {Squeezed Attention: Accelerating Long Context Length LLM Inference},
  journal   = {arXiv preprint arXiv:2411.09688},
  year      = {2025},
  url       = {https://arxiv.org/abs/2411.09688}
}

@article{dettmers2023qlora,
  author    = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
  title     = {{QL}o{RA}: Efficient Finetuning of Quantized {LLM}s},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {36},
  year      = {2023},
  url       = {https://arxiv.org/abs/2305.14314}
}

@article{chen2025survey,
  author    = {Chen, Danqi and others},
  title     = {A Comprehensive Survey on Long Context Language Modeling},
  journal   = {arXiv preprint arXiv:2503.17407},
  year      = {2025},
  url       = {https://arxiv.org/abs/2503.17407}
}

@article{liu2024lost,
  author    = {Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
  title     = {Lost in the Middle: How Language Models Use Long Contexts},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {12},
  pages     = {157--173},
  year      = {2024},
  url       = {https://arxiv.org/abs/2307.03172}
}

@article{luo2023empirical,
  author    = {Hongyin Luo and Tianhua Tao and Guangyun Wang and Yupeng Li and Congkai Zeng and Xiyao Ma and Wenjie Li},
  title     = {An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine Tuning},
  journal   = {arXiv preprint arXiv:2308.08747},
  year      = {2023},
  url       = {https://arxiv.org/abs/2308.08747}
}

@article{jiang2024long,
  author    = {Zihan Jiang and Mosh Levy and Alon Albalak and Ximing Lu and Haibin Lin and Hechang Chen and Shuming Ma and Junyu Mao and Jianqiang Huang and Wenda Xu and Andrew Dai and Dan Roth and Michael Jordan and Denny Zhou and Yejin Choi and Yong Jae Lee},
  title     = {Long-Context {LLM}s Meet {RAG}: Overcoming Challenges for Long Inputs in Retrieval-Augmented Generation},
  journal   = {arXiv preprint arXiv:2410.04564},
  year      = {2024},
  url       = {https://arxiv.org/abs/2410.04564}
}

@article{hu2021lora,
  author    = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  title     = {{LoRA}: Low-Rank Adaptation of Large Language Models},
  journal   = {arXiv preprint arXiv:2106.09685},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.09685}
}

@article{ouyang2022training,
  author    = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and others},
  title     = {Training language models to follow instructions with human feedback},
  journal   = {arXiv preprint arXiv:2203.02155},
  year      = {2022},
  url       = {https://arxiv.org/abs/2203.02155}
}

@article{gao2023retrieve,
  author    = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Haofen Wang},
  title     = {Retrieval-Augmented Generation for Large Language Models: A Survey},
  journal   = {arXiv preprint arXiv:2312.10997},
  year      = {2023},
  url       = {https://arxiv.org/abs/2312.10997}
}

@article{asai2023self,
  author    = {Akari Asai and Sewon Min and Zexuan Zhong and Danqi Chen},
  title     = {Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  journal   = {arXiv preprint arXiv:2310.11511},
  year      = {2023},
  url       = {https://arxiv.org/abs/2310.11511}
}

@article{ji2023survey,
  author    = {Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Ye Jin Bang and Andrea Madotto and Pascale Fung},
  title     = {Survey of Hallucination in Natural Language Generation},
  journal   = {ACM Computing Surveys},
  volume    = {55},
  number    = {12},
  pages     = {1--38},
  year      = {2023},
  url       = {https://arxiv.org/abs/2202.03629}
}

@misc{pinecone2025comparison,
  author    = {{Pinecone Team}},
  title     = {Vector Database Comparison: Pinecone vs Weaviate vs Qdrant vs Milvus},
  year      = {2025},
  howpublished = {\url{https://www.pinecone.io/blog/vector-database-comparison/}},
  note      = {Accessed: January 2026}
}

@misc{milvus2025,
  author    = {{Zilliz Team}},
  title     = {Milvus: High-Performance Vector Database for RAG},
  year      = {2025},
  howpublished = {\url{https://milvus.io}},
  note      = {Accessed: January 2026}
}

@article{johnson2019billion,
  author    = {Jeff Johnson and Matthijs Douze and Herv{\'e} J{\'e}gou},
  title     = {Billion-Scale Similarity Search with {GPUs}},
  journal   = {IEEE Transactions on Big Data},
  volume    = {7},
  number    = {3},
  pages     = {535--547},
  year      = {2019},
  publisher = {IEEE},
  doi       = {10.1109/TBDATA.2019.2907628},
  url       = {https://arxiv.org/abs/1702.08734}
}

@inproceedings{wang2021milvus,
  author    = {Jianguo Wang and Xiaomeng Yi and Rentong Guo and Hai Jin and Peng Xu and Shengjun Li and Xiangyu Wang and Xiangzhou Guo and Chengming Li and Xiaohai Xu and Kun Yu and Yuxing Yuan and Yinghao Zou and Jiquan Long and Yudong Cai and Zhenxiang Li and Zhifeng Zhang and Yihua Mo and Jun Gu and Ruiyi Jiang and Yi Wei and Charles Xie},
  title     = {Milvus: A Purpose-Built Vector Data Management System},
  booktitle = {Proceedings of the 2021 International Conference on Management of Data},
  series    = {SIGMOD '21},
  year      = {2021},
  pages     = {2614--2627},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/3448016.3457550},
  url       = {https://doi.org/10.1145/3448016.3457550}
}

@article{guo2022vector,
  author    = {Rentong Guo and Xiaofan Luan and Long Xiang and Xiao Yan and Xiaomeng Yi and Jigao Luo and Qianya Cheng and Weizhi Xu and Jiarui Luo and Frank Liu and Zhenshan Cao and Yanliang Qiao and Ting Wang and Bo Tang and Charles Xie},
  title     = {Manu: A Cloud Native Vector Database Management System},
  journal   = {Proceedings of the VLDB Endowment},
  volume    = {15},
  number    = {12},
  pages     = {3548--3561},
  year      = {2022},
  doi       = {10.14778/3554821.3554843},
  url       = {https://arxiv.org/abs/2206.13843}
}

@article{su2021roformer,
  author    = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
  title     = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  journal   = {Neurocomputing},
  volume    = {568},
  pages     = {127063},
  year      = {2023},
  doi       = {10.1016/j.neucom.2023.127063},
  url       = {https://www.sciencedirect.com/science/article/abs/pii/S0925231223011864}
}

@article{kirkpatrick2017overcoming,
  author    = {James Kirkpatrick and Razvan Pascanu and Neil Rabinowitz and Joel Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and Agnieszka Grabska-Barwinska and Demis Hassabis and Claudia Clopath and Dharshan Kumaran and Raia Hadsell},
  title     = {Overcoming catastrophic forgetting in neural networks},
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {114},
  number    = {13},
  pages     = {3521--3526},
  year      = {2017},
  doi       = {10.1073/pnas.1611835114},
  url       = {https://arxiv.org/abs/1612.00796}
}